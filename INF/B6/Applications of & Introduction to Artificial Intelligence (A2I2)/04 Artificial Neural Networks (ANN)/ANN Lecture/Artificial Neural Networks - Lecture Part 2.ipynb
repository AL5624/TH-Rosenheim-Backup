{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/thro.png\" align=\"right\"> \n",
    "# A2I2 - Artificial Neural Networks (ANN)\n",
    "\n",
    "## Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "After working through the homework notebooks 1 to 3 you are now familiar with artifical neurons, artifical neural networks, the feed forward algorithms, gradient descent, the backpropagation algorithm and you know how alcohol can make training ANNs much faster (the drunk man stumbling down the hill of stochastic gradient descent).\n",
    "\n",
    "### Any questions regarding this material?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of this Lecture\n",
    "\n",
    "**1) More on the cost function**\n",
    "\n",
    "**2) Optimizations regarding the training algorithm**\n",
    "\n",
    "**3) Training in batches and epochs**\n",
    "\n",
    "**4) Popular neural network setups for different problems***\n",
    "\n",
    "**5) MNIST - code example**\n",
    "\n",
    "**6) Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) More on the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the videos you learned about the ***cost function*** (which is often also called ***loss function*** or ***objective function***). Apart from the (mean) ***squared error*** that was introduced in the video, there is another very popular loss function, the ***cross-entropy*** (sometimes also called ***log loss***), which measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .017 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "\n",
    "Cross-entropy for a training example $h$ is defined as\n",
    "\\begin{equation*}\n",
    "\\epsilon(h)= - \\sum_j y_j(h) \\mathrm{ld}(d_j(h))\n",
    "\\end{equation*}\n",
    "\n",
    "where $d_j(h)$ is the output of neuron $j$ in the output layer and $y_j(h)$ is the desired output, which usually is $0$ for all but the one neuron encoding the correct class (for which is it $1$). This definition is derived from the definition of entropy in information theory. The original definition uses the logarithm to base 2 ($\\mathrm{ld}$), but any logarithm can be used, often the natural logarithm $\\ln$ to base $e$ is used.\n",
    "\n",
    "In the special case of binary classification, in which there is only one output neuron, the binare cross-entropy is used:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\epsilon(h)= - (y(h) \\mathrm{ld}(d(h)) + (1-y(h)) \\mathrm{ld}(1-d(h)))\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Optimizations regarding the training algorithm\n",
    "\n",
    "Instead of the simple ***SGD optimizer*** (stochastic gradient descent) introduced in the videos, most modern networks use more advanced ***optimizer*** algorithms. Two ideas in particular have been shown to be very beneficial:\n",
    "\n",
    "* **Adapting the learning rate during training**: SGD takes a step of a certain fixed size in the direction of the negative gradient. It has been observed, that it is good to make large steps at the beginnen of the training, when you are far away from the local minimum. Once you get closer to the minimum, it is better to make smaller steps in order to not \"overshoot\" the minimum. This can be achieved by adapting the learning rate dynamically during training.\n",
    "\n",
    "* **Using Moments**: The idea of this optimization is to not only use the current gradient as the direction of decents, but also factor in the gradients of the previous step with exponentially decreasing strength. One could say, instead of simple stumbling down the hill, our drunk man tries to stumble a little bit straighter. \n",
    "\n",
    "A number of optimizers have been proposed. Particularly popular is the **RMSprop optimizer** which combines both ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training in batches and epochs\n",
    "\n",
    "The video already introduced the concept of using only a subset (called a **batch** or **mini batch**) of the training examples to compute the gradient, leading to the SGD optimizer. Most neural network libraries divide the training examples into three sets: the **test set** to be used for evaluation after training (about 10% of the data), the **training set** used for actual training (about 80% of the data) and the **validation set** for validation during the training (about 10% of the data), resulting in the following process:\n",
    "\n",
    "1) divide the *training set* into the batches (e.g. 128 training examples per batch)\n",
    "\n",
    "2) train the network on one batch\n",
    "\n",
    "3) once all batches have been used for training (which is called an **epoch**), compute loss and some quality metric (e.g. accuracy) based on the *validation set*\n",
    "\n",
    "4) stop the training if either the maximal number of epochs has been reached or the quality metric has reached a given threshold, otherwise repeat from step 2\n",
    "\n",
    "After the training, compute the quality metric (and maybe the loss) on the *test set* in order to get a more realstic estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Popular neural network setups for different problems\n",
    "\n",
    "Depending on the problem at hand, different choices for the activation function of the output layer and the loss function can be recommended:\n",
    "\n",
    "**binary classification**: the output layer consists of only one neuron, which is supposed to be $0$ or $1$. Sigmoid as activation function and binary cross-entropy as loss function often work quite well.\n",
    "\n",
    "**multiple disjoint classes**: the output layer has one neuron per class. Exactly one of these is supposed to be $1$, all others should be $0$. Softmax as activation function and cross-entropy as loss function often work quite well.\n",
    "\n",
    "**multiple non-disjoint classes**: the output layer again has one neuron per class, but now multiple neurons can and should be $1$. Sigmoid as activation function and the sum of binary cross-entropy as loss function (each neuron can be considered a binary classfication problem) often work well.\n",
    "\n",
    "**regression**: instead of predicting a class label, the network has to compute a (numeric) function value. The output layer consists of one neuron. The identity function as activation function and the squared error as loss function often work well.\n",
    "\n",
    "For the hidden layers, pretty much all networks today use ReLU or one of its variants (leakyReLU, Softplus) as activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) MNIST - code example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Tensorflow/Keras, a very popular library for building neural networks. You can find the Keras documentation at https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using Azure Notebooks, please make sure to use a kernel that \n",
    "# includes tensorflow v2 or larger (e.g. the Python 3.6 kernel)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "from cycler import cycler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and split into train/test\n",
    "mnist = keras.datasets.mnist\n",
    "num_classes = 10\n",
    "print('Loading...')\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('...finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few digits...\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Digit: {}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# munge the data\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval(model, history):\n",
    "    # evalute the model quality\n",
    "    epochs = len(history.history['loss'])\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    # plot the metrics\n",
    "    # Create cycler object for the graphs. Use any styling you please\n",
    "    monochrome = (cycler('color', ['k']) * cycler('linestyle', ['-', '--', ':', '=.']))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_prop_cycle(monochrome)\n",
    "    plt.plot(history.history['accuracy'],linestyle='--')\n",
    "    plt.plot(history.history['val_accuracy'],linestyle='-')\n",
    "    startacc = min([min(history.history['accuracy']), min(history.history['val_accuracy'])])\n",
    "    plt.ylim([startacc, 1])\n",
    "    plt.xticks(range(0,epochs))\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_prop_cycle(monochrome)\n",
    "    plt.plot(history.history['loss'],linestyle='--')\n",
    "    plt.plot(history.history['val_loss'],linestyle='-')\n",
    "    plt.xticks(range(0,epochs))\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A model very close to the one from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 48\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myeval(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Improving the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 48\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myeval(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Switch optimizer from SGD to RMSprop and loss function from mean squared error to (categorical) cross-entropy. How does the quality of the network change? Discuss/interpret your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Increase the number of neurons in the hidden layers to 256 and the epochs to 128. Again, how does the quality of the network change? Discuss/interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EOF ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot exercise.ipynb","provenance":[],"authorship_tag":"ABX9TyNSoJoMS8f+kiAF0CF2ojls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ussoahpAU"},"source":["# Build your own Chatbot\n","\n","Remember the format of the json you have to provide. Come up with 3 intents (maybe you can use it from Zumpad) and provide yout sample utterances.\n","\n","Format:\n","\n","```\n","{\"intents\": [\n","  {\"tag\": \"greeting\",\n","    \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Whats up\", \"Hey\", \"greetings\"],\n","    \"responses\": [\"Hello!\", \"Good to see you again!\", \"Hi there, how can I help?\"],\n","    \"context_set\": \"\"\n","  },\n","  {\"tag\": \"goodbye\",\n","    \"patterns\": [\"cya\", \"See you later\", \"Goodbye\", \"I am Leaving\", \"Have a Good day\", \"bye\", \"cao\", \"see ya\"],\n","    \"responses\": [\"Sad to see you go :(\", \"Talk to you later\", \"Goodbye!\"],\n","    \"context_set\": \"\"\n","  },\n","  {\"tag\": \"stocks\",\n","    \"patterns\": [\"what stocks do I own?\", \"how are my shares?\", \"what companies am I investing in?\", \"what am I doing in the markets?\"],\n","    \"responses\": [\"You own the following shares: ABBV, AAPL, FB, NVDA and an ETF of the S&P 500 Index!\"],\n","    \"context_set\": \"\"\n","  }\n","]\n","}\n","```"]},{"cell_type":"markdown","metadata":{"id":"xOvj3dIVq1G0"},"source":["## Task1:\n","\n","Prepare your language model. In this sample we use a bag-of-words approach.\n","\n","**Improve the stopword removal and punctuation removal by using the approach from last week!**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ylxa-cyPhntr","executionInfo":{"status":"ok","timestamp":1620723733906,"user_tz":-120,"elapsed":624,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"619cdeac-24be-4643-89d9-0ae1678112c6"},"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","import os\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","intents = json.loads(open(\"intents.json\").read())\n","\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['!', '?', ',', '.']\n","\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        word = nltk.word_tokenize(pattern)\n","        words.extend(word)\n","        documents.append((word, intent['tag']))\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# TODO:potential for stopword removal and punctuation removal improvements\n","words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]\n","words = sorted(list(set(words)))\n","\n","classes = sorted(list(set(classes)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VwFT62ljdTO","executionInfo":{"status":"ok","timestamp":1620723733907,"user_tz":-120,"elapsed":606,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"ad5b6ed6-f0b6-40ad-b210-71e20a1a4038"},"source":["print(words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['a', 'am', 'anyone', 'are', 'bye', 'cao', 'company', 'cya', 'day', 'do', 'doing', 'good', 'goodbye', 'greeting', 'have', 'hello', 'hey', 'hi', 'how', 'i', 'in', 'investing', 'is', 'later', 'leaving', 'market', 'my', 'own', 'see', 'share', 'stock', 'the', 'there', 'up', 'what', 'whats', 'ya', 'you']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sgO1LNkjeqP","executionInfo":{"status":"ok","timestamp":1620723733908,"user_tz":-120,"elapsed":600,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"4b27ea00-b33d-4e7f-ed0e-22b55e4a6a67"},"source":["print(classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['goodbye', 'greeting', 'stocks']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RTBVTIDj926","executionInfo":{"status":"ok","timestamp":1620723733909,"user_tz":-120,"elapsed":595,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"271b6e51-abb8-4760-b2e4-41dd2fda074c"},"source":["print(documents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(['Hi'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Whats', 'up'], 'greeting'), (['Hey'], 'greeting'), (['greetings'], 'greeting'), (['cya'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['I', 'am', 'Leaving'], 'goodbye'), (['Have', 'a', 'Good', 'day'], 'goodbye'), (['bye'], 'goodbye'), (['cao'], 'goodbye'), (['see', 'ya'], 'goodbye'), (['what', 'stocks', 'do', 'I', 'own', '?'], 'stocks'), (['how', 'are', 'my', 'shares', '?'], 'stocks'), (['what', 'companies', 'am', 'I', 'investing', 'in', '?'], 'stocks'), (['what', 'am', 'I', 'doing', 'in', 'the', 'markets', '?'], 'stocks')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p3yZNwEbrLY4"},"source":["## Task2:\n","Prepare your bag-of-words in terms of embeddings to get it trained by a neural network.\n","\n","We do have 3 classes [greetings, goodbye, stocks], which get mapped to [[1,0,0],[0,1,0],[0,0,1]] for the output.\n","\n","The input layer receives the length of the vocabulary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYtumGQViCbK","executionInfo":{"status":"ok","timestamp":1620723734591,"user_tz":-120,"elapsed":1270,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"1eb50ff1-384d-4794-b191-7e4b4f187345"},"source":["training = []\n","output_empty = [0] * len(classes)\n","\n","for doc in documents:\n","    bag = []\n","    word_patterns = doc[0]\n","    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","    for word in words:\n","        bag.append(1) if word in word_patterns else bag.append(0)\n","\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","    training.append([bag, output_row])\n","\n","print(training)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0]], [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1]], [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1]], [[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1]], [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 1]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w5g2yt_nlfWj"},"source":["### Task3:\n","\n","Now, lets train a neural network for this. For this we have to define x and y, a couple of layers and hyperparameter as you can see in the code below.\n","\n","**Chage the hyperparameter to improve your mode**\n","- Change the epochs\n","- or the loss function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_7T48GBkZZy","executionInfo":{"status":"ok","timestamp":1620723739043,"user_tz":-120,"elapsed":5718,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"743d096c-632e-49f9-b9ac-58967e1f8bea"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.models import load_model\n","\n","random.shuffle(training)\n","training = np.array(training)\n","\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])\n","\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","# TODO: Change and adjust the hyperparameter\n","model.compile(loss='categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","\n","# in case we do not want to repeat training, we can save the results\n","model.save(\"intents.h5\", hist)\n","pickle.dump(words, open('words.pkl', 'wb'))\n","pickle.dump(classes, open('classes.pkl', 'wb'))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  import sys\n"],"name":"stderr"},{"output_type":"stream","text":["4/4 [==============================] - 0s 3ms/step - loss: 1.0868 - accuracy: 0.3467\n","Epoch 2/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.1242 - accuracy: 0.1600\n","Epoch 3/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0642 - accuracy: 0.4400\n","Epoch 4/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.1573 - accuracy: 0.2333\n","Epoch 5/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0970 - accuracy: 0.3800\n","Epoch 6/200\n","4/4 [==============================] - 0s 5ms/step - loss: 1.0772 - accuracy: 0.2867\n","Epoch 7/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.1397 - accuracy: 0.2933\n","Epoch 8/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.1218 - accuracy: 0.3200\n","Epoch 9/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.1254 - accuracy: 0.3867\n","Epoch 10/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0556 - accuracy: 0.4800\n","Epoch 11/200\n","4/4 [==============================] - 0s 5ms/step - loss: 1.0583 - accuracy: 0.4000\n","Epoch 12/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9909 - accuracy: 0.5467\n","Epoch 13/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.0314 - accuracy: 0.4400\n","Epoch 14/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.9934 - accuracy: 0.6733\n","Epoch 15/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.1232 - accuracy: 0.3800\n","Epoch 16/200\n","4/4 [==============================] - 0s 5ms/step - loss: 1.0929 - accuracy: 0.4667\n","Epoch 17/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0017 - accuracy: 0.5267\n","Epoch 18/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.1018 - accuracy: 0.4600\n","Epoch 19/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.1195 - accuracy: 0.2800\n","Epoch 20/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.0005 - accuracy: 0.5267\n","Epoch 21/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.0182 - accuracy: 0.4733\n","Epoch 22/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0115 - accuracy: 0.5667\n","Epoch 23/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9989 - accuracy: 0.5800\n","Epoch 24/200\n","4/4 [==============================] - 0s 3ms/step - loss: 1.0191 - accuracy: 0.5933\n","Epoch 25/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9961 - accuracy: 0.4400\n","Epoch 26/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9637 - accuracy: 0.4933\n","Epoch 27/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.9407 - accuracy: 0.6000\n","Epoch 28/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9814 - accuracy: 0.6733\n","Epoch 29/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9661 - accuracy: 0.7067\n","Epoch 30/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9549 - accuracy: 0.6533\n","Epoch 31/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0211 - accuracy: 0.4067\n","Epoch 32/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9150 - accuracy: 0.6533\n","Epoch 33/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0060 - accuracy: 0.4533\n","Epoch 34/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9561 - accuracy: 0.5800\n","Epoch 35/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9276 - accuracy: 0.6267\n","Epoch 36/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9721 - accuracy: 0.4933\n","Epoch 37/200\n","4/4 [==============================] - 0s 4ms/step - loss: 1.0421 - accuracy: 0.5467\n","Epoch 38/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9365 - accuracy: 0.6867\n","Epoch 39/200\n","4/4 [==============================] - 0s 5ms/step - loss: 1.0193 - accuracy: 0.4800\n","Epoch 40/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9230 - accuracy: 0.8000\n","Epoch 41/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9429 - accuracy: 0.6733\n","Epoch 42/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9519 - accuracy: 0.4667\n","Epoch 43/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8732 - accuracy: 0.8600\n","Epoch 44/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.8684 - accuracy: 0.7733\n","Epoch 45/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9414 - accuracy: 0.6933\n","Epoch 46/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8669 - accuracy: 0.6133\n","Epoch 47/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.9178 - accuracy: 0.5133\n","Epoch 48/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9173 - accuracy: 0.7133\n","Epoch 49/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.9515 - accuracy: 0.6933\n","Epoch 50/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9066 - accuracy: 0.5267\n","Epoch 51/200\n","4/4 [==============================] - 0s 7ms/step - loss: 0.8703 - accuracy: 0.6600\n","Epoch 52/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9117 - accuracy: 0.6933\n","Epoch 53/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.9472 - accuracy: 0.7000\n","Epoch 54/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.8597 - accuracy: 0.6600\n","Epoch 55/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8527 - accuracy: 0.7067\n","Epoch 56/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8693 - accuracy: 0.7267\n","Epoch 57/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.8729 - accuracy: 0.8267\n","Epoch 58/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.9032 - accuracy: 0.5400\n","Epoch 59/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.9232 - accuracy: 0.5467\n","Epoch 60/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8850 - accuracy: 0.6933\n","Epoch 61/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9384 - accuracy: 0.5800\n","Epoch 62/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.9490 - accuracy: 0.5200\n","Epoch 63/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7502 - accuracy: 0.8733\n","Epoch 64/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.8577 - accuracy: 0.7467\n","Epoch 65/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.8420 - accuracy: 0.5867\n","Epoch 66/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.8786 - accuracy: 0.6333\n","Epoch 67/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8506 - accuracy: 0.6133\n","Epoch 68/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8942 - accuracy: 0.5667\n","Epoch 69/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.8080 - accuracy: 0.8400\n","Epoch 70/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7668 - accuracy: 0.7933\n","Epoch 71/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.7459 - accuracy: 0.7667\n","Epoch 72/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8310 - accuracy: 0.8000\n","Epoch 73/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.9363 - accuracy: 0.6200\n","Epoch 74/200\n","4/4 [==============================] - 0s 8ms/step - loss: 0.7874 - accuracy: 0.8133\n","Epoch 75/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7862 - accuracy: 0.6400\n","Epoch 76/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.8036 - accuracy: 0.8733\n","Epoch 77/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.8132 - accuracy: 0.7200\n","Epoch 78/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.8164 - accuracy: 0.7067\n","Epoch 79/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.7827 - accuracy: 0.8533\n","Epoch 80/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7571 - accuracy: 0.8933\n","Epoch 81/200\n","4/4 [==============================] - 0s 8ms/step - loss: 0.7101 - accuracy: 0.8933\n","Epoch 82/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8733 - accuracy: 0.6667\n","Epoch 83/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8512 - accuracy: 0.6133\n","Epoch 84/200\n","4/4 [==============================] - 0s 2ms/step - loss: 0.7769 - accuracy: 0.7667\n","Epoch 85/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.7461 - accuracy: 0.8200\n","Epoch 86/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7156 - accuracy: 0.8733\n","Epoch 87/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.7418 - accuracy: 0.6933\n","Epoch 88/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.7933\n","Epoch 89/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7079 - accuracy: 0.8200\n","Epoch 90/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6490 - accuracy: 0.7667\n","Epoch 91/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.9467\n","Epoch 92/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8085 - accuracy: 0.7800\n","Epoch 93/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.7968 - accuracy: 0.8000\n","Epoch 94/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7284 - accuracy: 0.8200\n","Epoch 95/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.7781 - accuracy: 0.6733\n","Epoch 96/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.7123 - accuracy: 0.7200\n","Epoch 97/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6480 - accuracy: 0.8067\n","Epoch 98/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6322 - accuracy: 0.8733\n","Epoch 99/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.8333\n","Epoch 100/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.8733\n","Epoch 101/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.8630 - accuracy: 0.7067\n","Epoch 102/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.9667\n","Epoch 103/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.9800\n","Epoch 104/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6139 - accuracy: 0.8000\n","Epoch 105/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.7184 - accuracy: 0.7800\n","Epoch 106/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5560 - accuracy: 0.9133\n","Epoch 107/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.7385 - accuracy: 0.6933\n","Epoch 108/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5534 - accuracy: 0.8000\n","Epoch 109/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.9133\n","Epoch 110/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5245 - accuracy: 0.8933\n","Epoch 111/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6606 - accuracy: 0.7667\n","Epoch 112/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.9667\n","Epoch 113/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5510 - accuracy: 0.7733\n","Epoch 114/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5492 - accuracy: 0.9667\n","Epoch 115/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.9067\n","Epoch 116/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6275 - accuracy: 0.7933\n","Epoch 117/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5709 - accuracy: 0.8933\n","Epoch 118/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5777 - accuracy: 0.9067\n","Epoch 119/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.8333\n","Epoch 120/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5508 - accuracy: 0.9267\n","Epoch 121/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5412 - accuracy: 0.8000\n","Epoch 122/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.6853 - accuracy: 0.6867\n","Epoch 123/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5834 - accuracy: 0.8200\n","Epoch 124/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5301 - accuracy: 0.7933\n","Epoch 125/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.8600\n","Epoch 126/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5692 - accuracy: 0.8933\n","Epoch 127/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.6430 - accuracy: 0.8333\n","Epoch 128/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.4749 - accuracy: 0.8400\n","Epoch 129/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5446 - accuracy: 0.9067\n","Epoch 130/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5173 - accuracy: 1.0000\n","Epoch 131/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.9067\n","Epoch 132/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5428 - accuracy: 0.8600\n","Epoch 133/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4885 - accuracy: 0.8533\n","Epoch 134/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4365 - accuracy: 0.9133\n","Epoch 135/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.4979 - accuracy: 0.9467\n","Epoch 136/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5212 - accuracy: 0.7600\n","Epoch 137/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.9267\n","Epoch 138/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.4140 - accuracy: 1.0000\n","Epoch 139/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4900 - accuracy: 0.8533\n","Epoch 140/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.8733\n","Epoch 141/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5029 - accuracy: 0.8200\n","Epoch 142/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.9267\n","Epoch 143/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5266 - accuracy: 0.8400\n","Epoch 144/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8533\n","Epoch 145/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5592 - accuracy: 0.8200\n","Epoch 146/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8200\n","Epoch 147/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.8600\n","Epoch 148/200\n","4/4 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.9133\n","Epoch 149/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.9667\n","Epoch 150/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.4444 - accuracy: 0.9467\n","Epoch 151/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4271 - accuracy: 0.8867\n","Epoch 152/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4370 - accuracy: 0.9067\n","Epoch 153/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5619 - accuracy: 0.7067\n","Epoch 154/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.4609 - accuracy: 1.0000\n","Epoch 155/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.3731 - accuracy: 0.9467\n","Epoch 156/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4102 - accuracy: 0.8867\n","Epoch 157/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.4742 - accuracy: 0.8533\n","Epoch 158/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.9467\n","Epoch 159/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5248 - accuracy: 0.7800\n","Epoch 160/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.8733\n","Epoch 161/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5952 - accuracy: 0.8133\n","Epoch 162/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.9600\n","Epoch 163/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.5003 - accuracy: 0.8000\n","Epoch 164/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.9600\n","Epoch 165/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3641 - accuracy: 0.9667\n","Epoch 166/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.9133\n","Epoch 167/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.9467\n","Epoch 168/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.9067\n","Epoch 169/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.9267\n","Epoch 170/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3955 - accuracy: 0.9667\n","Epoch 171/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.9067\n","Epoch 172/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.9800\n","Epoch 173/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.9800\n","Epoch 174/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.9800\n","Epoch 175/200\n","4/4 [==============================] - 0s 5ms/step - loss: 0.4394 - accuracy: 0.8933\n","Epoch 176/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3072 - accuracy: 0.9467\n","Epoch 177/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 1.0000\n","Epoch 178/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8000\n","Epoch 179/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.4167 - accuracy: 0.9067\n","Epoch 180/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.5770 - accuracy: 0.8000\n","Epoch 181/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 1.0000\n","Epoch 182/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4443 - accuracy: 0.8133\n","Epoch 183/200\n","4/4 [==============================] - 0s 6ms/step - loss: 0.3566 - accuracy: 0.9800\n","Epoch 184/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.9667\n","Epoch 185/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 1.0000\n","Epoch 186/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8933\n","Epoch 187/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9467\n","Epoch 188/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 1.0000\n","Epoch 189/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.9800\n","Epoch 190/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 1.0000\n","Epoch 191/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3479 - accuracy: 1.0000\n","Epoch 192/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3254 - accuracy: 0.9067\n","Epoch 193/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.9800\n","Epoch 194/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8533\n","Epoch 195/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.9067\n","Epoch 196/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8200\n","Epoch 197/200\n","4/4 [==============================] - 0s 15ms/step - loss: 0.2703 - accuracy: 1.0000\n","Epoch 198/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3622 - accuracy: 0.8867\n","Epoch 199/200\n","4/4 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.9467\n","Epoch 200/200\n","4/4 [==============================] - 0s 4ms/step - loss: 0.3106 - accuracy: 0.9600\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XO_Gia2nmS1T"},"source":["## Task4:\n","\n","Let's build the chatbot code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3N_zOJmmTOT","executionInfo":{"status":"ok","timestamp":1620723769969,"user_tz":-120,"elapsed":36639,"user":{"displayName":"Marcel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnqgvB1-dXF9xtSsw7hpY5o7BOISI-Okj9toVN=s64","userId":"00210246741827932529"}},"outputId":"5145ff08-c518-4000-c60a-308c87b0e706"},"source":["# uncommment this if you want to load the data\n","\n","#  words = pickle.load(open('words.pkl', 'rb'))\n","#  classes = pickle.load(open('classes.pkl', 'rb'))\n","#  model = load_model('intents.h5')\n","\n","def predict(sentence):\n","  sentence_words = nltk.word_tokenize(sentence)\n","  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","\n","  bag = [0] * len(words)\n","  for s in sentence_words:\n","      for i, word in enumerate(words):\n","          if word == s:\n","              bag[i] = 1\n","\n","  res = model.predict(np.array([bag]))[0]\n","  ERROR_THRESHOLD = 0.1\n","  results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n","\n","  results.sort(key=lambda x: x[1], reverse=True)\n","  return_list = []\n","  for r in results:\n","      return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n","  return return_list\n","\n","def request(message):\n","  ints = predict(message)\n","\n","  print(ints)\n","  try:\n","    tag = ints[0]['intent']\n","    list_of_intents = intents['intents']\n","    for i in list_of_intents:\n","        if i['tag']  == tag:\n","            result = random.choice(i['responses'])\n","            break\n","  except IndexError:\n","    result = \"I don't understand!\"\n","  return result\n","\n","\n","# TODO: MAybe you want to improve the output!!!!\n","done = False;\n","while not done:\n","    message = input(\"Enter a message: \")\n","    if message == \"STOP\":\n","        done = True\n","    else:\n","        print(request(message))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Enter a message: Hi\n","WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f54851570e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","[{'intent': 'greeting', 'probability': '0.8099524'}, {'intent': 'goodbye', 'probability': '0.13175413'}]\n","Good to see you again!\n","Enter a message: Bye\n","[{'intent': 'goodbye', 'probability': '0.78330916'}, {'intent': 'greeting', 'probability': '0.18973133'}]\n","Talk to you later\n","Enter a message: STOP\n"],"name":"stdout"}]}]}
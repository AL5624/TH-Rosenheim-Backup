{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Textgeneration with RNNs\n\nActually, there is a phantastic [blob post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) about \"The Unreasonable Effectiveness of Recurrent Neural Networks\". He is pointing to some code to generate some charakter elvel language-models.\n\nI believe the code is coming from there.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#implemented as I read Andrej Karpathy's post on RNNs.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass RNN(object):\n\n    def __init__(self, insize, outsize, hidsize, learning_rate):        \n        self.insize = insize\n\n        self.h = np.zeros((hidsize , 1))#a [h x 1] hidden state stored from last batch of inputs\n\n        #parameters\n        self.W_hh = np.random.randn(hidsize, hidsize)*0.01#[h x h]\n        self.W_xh = np.random.randn(hidsize, insize)*0.01#[h x x]\n        self.W_hy = np.random.randn(outsize, hidsize)*0.01#[y x h]\n        self.b_h = np.zeros((hidsize, 1))#biases\n        self.b_y = np.zeros((outsize, 1))\n\n        #the Adagrad gradient update relies upon having a memory of the sum of squares of dparams\n        self.adaW_hh = np.zeros((hidsize, hidsize))\n        self.adaW_xh = np.zeros((hidsize, insize))\n        self.adaW_hy = np.zeros((outsize, hidsize))\n        self.adab_h = np.zeros((hidsize, 1))\n        self.adab_y = np.zeros((outsize, 1))\n\n        self.learning_rate = learning_rate\n\n    #give the RNN a sequence of inputs and outputs (seq_length long), and use\n    #them to adjust the internal state\n    def train(self, x, y):\n        #=====initialize=====\n        xhat = {}#holds 1-of-k representations of x\n        yhat = {}#holds 1-of-k representations of predicted y (unnormalized log probs)\n        p = {}#the normalized probabilities of each output through time\n        h = {}#holds state vectors through time\n        h[-1] = np.copy(self.h)#we will need to access the previous state to calculate the current state\n\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(self.h)\n\n        #=====forward pass=====\n        loss = 0\n        for t in range(len(x)):\n            xhat[t] = np.zeros((self.insize, 1))\n            xhat[t][x[t]] = 1#xhat[t] = 1-of-k representation of x[t]\n\n            h[t] = np.tanh(np.dot(self.W_xh, xhat[t]) + np.dot(self.W_hh, h[t-1]) + self.b_h)#find new hidden state\n            yhat[t] = np.dot(self.W_hy, h[t]) + self.b_y#find unnormalized log probabilities for next chars\n\n            p[t] = np.exp(yhat[t]) / np.sum(np.exp(yhat[t]))#find probabilities for next chars\n\n            loss += -np.log(p[t][y[t],0])#softmax (cross-entropy loss)\n\n        #=====backward pass: compute gradients going backwards=====\n        for t in reversed(range(len(x))):\n            #backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n            dy = np.copy(p[t])\n            dy[y[t]] -= 1\n\n            #find updates for y\n            dW_hy += np.dot(dy, h[t].T)\n            db_y += dy\n\n            #backprop into h and through tanh nonlinearity\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - h[t]**2) * dh\n\n            #find updates for h\n            dW_xh += np.dot(dh_raw, xhat[t].T)\n            dW_hh += np.dot(dh_raw, h[t-1].T)\n            db_h += dh_raw\n\n            #save dh_next for subsequent iteration\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)#clip to mitigate exploding gradients\n\n        #update RNN parameters according to Adagrad\n        for param, dparam, adaparam in zip([self.W_hh, self.W_xh, self.W_hy, self.b_h, self.b_y], \\\n                                [dW_hh, dW_xh, dW_hy, db_h, db_y], \\\n                                [self.adaW_hh, self.adaW_xh, self.adaW_hy, self.adab_h, self.adab_y]):\n            adaparam += dparam*dparam\n            param += -self.learning_rate*dparam/np.sqrt(adaparam+1e-8)\n\n        self.h = h[len(x)-1]\n\n        return loss\n\n    #let the RNN generate text\n    def sample(self, seed, n):\n        ndxs = []\n        h = self.h\n\n        xhat = np.zeros((self.insize, 1))\n        xhat[seed] = 1#transform to 1-of-k\n\n        for t in range(n):\n            h = np.tanh(np.dot(self.W_xh, xhat) + np.dot(self.W_hh, h) + self.b_h)#update the state\n            y = np.dot(self.W_hy, h) + self.b_y\n            p = np.exp(y) / np.sum(np.exp(y))\n            ndx = np.random.choice(range(self.insize), p=p.ravel())\n\n            xhat = np.zeros((self.insize, 1))\n            xhat[ndx] = 1\n\n            ndxs.append(ndx)\n\n        return ndxs\n\n\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Testing\n\nFor testing purposes I have downloaded some text of a Sonnet written by Shakespeare which was available as TXT."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#open a text file\ndata = open('input.txt', 'r').read() # should be simple plain text file\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint 'data has %d characters, %d unique.' % (data_size, vocab_size)\n\n#make some dictionaries for encoding and decoding from 1-of-k\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }\n\n#insize and outsize are len(chars). hidsize is 100. seq_length is 25. learning_rate is 0.1.\nrnn = RNN(len(chars), len(chars), 100, 0.1)\n\n#iterate over batches of input and target output\nseq_length = 25\nlosses = []\nsmooth_loss = -np.log(1.0/len(chars))*seq_length#loss at iteration 0\nlosses.append(smooth_loss)\n\nfor i in range(len(data)/seq_length):\n    x = [char_to_ix[c] for c in data[i*seq_length:(i+1)*seq_length]]#inputs to the RNN\n    y = [char_to_ix[c] for c in data[i*seq_length+1:(i+1)*seq_length+1]]#the targets it should be outputting\n\n    if i%1000==0:\n        sample_ix = rnn.sample(x[0], 200)\n        txt = ''.join([ix_to_char[n] for n in sample_ix])\n        print txt\n\n    loss = rnn.train(x, y)\n    smooth_loss = smooth_loss*0.999 + loss*0.001\n\n    if i%1000==0:\n        print 'iteration %d, smooth_loss = %f' % (i, smooth_loss)\n        losses.append(smooth_loss)\n\nplt.plot(range(len(losses)), losses, 'b', label='smooth loss')\nplt.xlabel('time in thousands of iterations')\nplt.ylabel('loss')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "data has 5571357 characters, 85 unique.\n&gHvJ|:caB]lk&M3!MAc]&Cebj})GbcxbK_6-hHbUZtAx>We|AjJ'7V,>RXoOC3c8jJFB?EnxeC;nlACgN8jp:_[OroMufeAp}4)Da-,};U1c<mg1RS2p5plaPOoGr,]AhkAS\nBAVh>C|_:up'gUYRT,;[w_-,W)n1paMr;iF&}Ck8)OruM9j6pEXU[G3uJ.]cnZziy(\niteration 0, smooth_loss = 111.066280\n pois mel's mlawtet tetyt sutd raicekiken' shoet oul hinte thechs dI e mie, cokeneks tiave meps ,rapd  s ronge so s r\n \n   aujt doecve,\n    or the ich th,uighos the thyarlove t\niteration 1000, smooth_loss = 83.370750\nt thy yocecut tin vei may,\n        n  we hrees I by wuct I ule soututj sth led ghess Ane sham ye se lee dhat algle ffe, shabet mens mam,\n  Fo s af Dose scare thins geanpest.\n  Then cest boil aog th\niteration 2000, smooth_loss = 64.936911\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}